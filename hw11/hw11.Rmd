---
title: "BACS HW11"
author: '109062710'
date: "5/5/2021"
output: html_document
---

```{r, include=FALSE}
library(dplyr)
library(ISLR)
library(tidyverse)
library(corrr)
library(PerformanceAnalytics)
```

## Question 1

#### a. Let’s dig into what regression is doing to compute model fit

##### i. Plot Scenario 2, storing the returned points
```{r}
pts <- data.frame(
  x = c(
    -4.704724, 3.966620, 3.448928, 11.861426, 11.473157,
    20.662193, 18.462001, 27.909883, 25.709691, 35.416420,
    32.957382, 41.887572, 41.369880, 49.652955, 7.331619
  ),
  y = c(
    4.682789, -1.593332, 14.096971, 7.472177, 22.465133,
    16.537685, 31.879314, 25.951867, 41.293496, 29.089927,
    45.826250, 39.550129, 50.010331, 48.615637, 5.728810
  )
)
```

##### ii. Run a linear model of x and y points to confirm the R2 value reported by the simulation

```{r}
regr <- lm(y ~ x, data = pts)
summary(regr)
```

#### iii. Add line segments to the plot 

1. Get values of $\hat{y}$ (estimated values)

```{r}
y_hat <- regr$fitted.values
y_hat
```

2. Add segments

```{r}
plot(pts)
abline(lm(pts$y ~ pts$x))
segments(pts$x, pts$y, pts$x, y_hat, col="red", lty="dotted")
```

##### iv. Use only `pts$x, pts$y, y_hat and mean(pts$y)` to compute SSE, SSR and SST, and verify $R^2$

```{r}
sse <- sum((fitted(regr) - mean(pts$y))^2)
ssr <- sum((fitted(regr) - pts$y)^2)
sst <- sse + ssr
r2 <- 1 - (ssr / sst)
```

#### b. Comparing scenarios 1 and 2, which do we expect to have a stronger $R^2$?

<p style="color: blue;">
For the first scenario, $R^2$ will be very close to $+1$ since most of the data points are sitting at or close to the increasing regression line. However, the second scenario's $R^2$ value won't be as high as the first scenario's, but it still will be near $1$.
</p>

<p style="color: red;">
In this case, the first scenario will have a stronger $R^2$.
</p>

#### c. Comparing scenarios 3 and 4, which do we expect to have a stronger $R^2$?

<p style="color: blue;">
In the third scenario, the $R^2$ will be close to $-1$ since most of the data points are sitting on or close to the decreasing regression line. However, the fourth scenario's $R^2$ value won't be as high as the third scenario's, but it still will be near $-1$.
</p>

<p style="color: red;">
In this case, the third scenario will have a stronger $R^2$, but in a decreasing manner.
</p>

#### d. Comparing scenarios 1 and 2, which do we expect has bigger/smaller SSE, SSR, and SST?

<p style="color: red;">
In first scenario, we expect smaller SSE & SST. Clearly, SST value for the first scenario will be also small in this case. 
</p>

<p style="color: red;">
In the second scenario, SSE will be bigger since all the data points are quite far from the regression line. However, the distance from the mean value might or might not be close to the regression line. Clearly, the second scenario will have bigger SST because of SSE, regardless SSR.
</p>

#### e. Comparing scenarios 3 and 4, which do we expect has bigger/smaller SSE, SSR, and SST?

<p style="color: red;">
The third scenario is quite similar to the first scenario since most data points are sitting quite close to the regression line. Thus, SST will be smaller compared to the fourth scenario due to small SSE and SSR values.
</p>

<p style="color: red;">
The fourth scenario is also similar to the second scenario where SSE is big since most data points are sitting far from the regression line. Clearly, SST value will be bigger compared to the third scenario due to SSE, regardless SSR value.
</p>

## Question 2

#### a. Let’s first try exploring this data and problem

```{r}
auto <- Auto
origins <- c("USA", "Europe", "Japan")
auto$origin <- factor(auto$origin, labels = origins)
```


##### i. Visualize the data in any way you feel relevant

```{r}
qplot(
  auto$origin,
  xlab = "Origin", 
  ylab = "Count", 
  main = "Distribution of Cars' Origin"
)
```

```{r}
qplot(auto$cylinders, xlab = 'Cylinders', ylab = 'Count', 
      main='Frequency Histogram: Number of Cylinders', binwidth = 1)
```

```{r}
qplot(auto$horsepower, xlab = 'Horsepower', ylab = 'Count', binwidth = 10,
      main='Frequency Histogram: Horsepower')
```

```{r}
ggplot(data = auto, aes(x = weight, y = mpg)) +
  geom_point() +
  geom_smooth(method = lm) +
  xlab('MPG') +
  ylab('Weight') +
  ggtitle('MPG vs. Weight: Entire Sample')
```


##### ii. Report a correlation table of all variables, rounding to two decimal places

```{r}
variables <- 
  auto[, 
       c("mpg", 
         "cylinders", 
         "displacement", 
         "horsepower", 
         "weight")
  ]
variables %>% 
  correlate() %>% 
  network_plot(min_cor = 0.7, colors = c("red", "green"), legend = TRUE)
```

```{r}
chart.Correlation(variables, histogram=TRUE, pch=19)
```

##### iii. From the visualizations and correlations, which variables seem to relate to mpg?

<p style="color: red;">
From the correlation graph, seems like `cylinders`, `displacement`, `horsepower`, and `weight` relare to `mpg`. However, they are negatively correlated. Besides, `weight` is the only variable with the correlation value closest $-1$ meaning it's the most negatively correlated variable against `mpg`.
</p>

##### iv. Which relationships might not be linear?

<p style="color: red;">
The relationships between `mpg` with `displacement`, `horsepower` and `weight` are not linear, indicated by the moon-shaped curve that the data points follow. Refer to the graph in (iii)
</p>

##### v. Are there any pairs of independent variables that are highly correlated r > 0.7?

<p style="color: red;">
Yes, there are `cylinders`, `displacement`, `horsepower` and `weight`. The values can be seen in (ii) and (iii) answers.
</p>

#### b. Let’s create a linear regression model where mpg is dependent upon all other suitable variables 

```{r}
simple_regr <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + factor(origin), data=auto)
summary(simple_regr)
```

##### i. Which independent variables have a ‘significant’ relationship with mpg at 1% significance?

<p style="color: red;">
There are five independent variables that have significant relationships with mpg at 1% significance, namely `displacement`, `weight`, `year`, `factor(origin)Europe`, and `factor(origin)Japan`.
</p>

##### ii. Looking at the coefficients, is it possible to determine which independent variables are the most effective at increasing mpg? If so, which ones, and if not, why not? 

<p style="color: red;">
We can't determine which and which are effective since all the independent variables are not standardized.
</p>

#### c. Let’s try to resolve some of the issues with our regression model above.

##### i. Create fully standardized regression results: are these slopes easier to compare? 

```{r}
auto_sd <- cbind(scale(auto[1:7]), auto$origin)
colnames(auto_sd)[8] <- "origin"
auto_sd <- as.data.frame(auto_sd)
auto_sd$origin <- factor(auto$origin, labels = origins)
simple_regr_std <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + factor(origin), data=auto_sd)
summary(simple_regr_std)
```

##### ii. Regress mpg over each non significant independent variable, individually. Which ones become significant when we regress mpg over them individually?

```{r}
mpg_cyl.lm <- lm(mpg ~ cylinders, data = auto_sd)
summary(mpg_cyl.lm)
```

```{r}
mpg_hp.lm <- lm(mpg ~ horsepower, data = auto_sd)
summary(mpg_hp.lm)
```


```{r}
mpg_acc.lm <- lm(mpg ~ acceleration, data = auto_sd)
summary(mpg_acc.lm)
```

<p style="color: red;">
All of them became significant.
</p>

##### iii. Plot the density of the residuals: are they normally distributed and centered around zero?

```{r}
mpg_cyl.res <- resid(mpg_cyl.lm)
plot(density(mpg_cyl.res), main = "Density Distribution of MPG ~ Cylinders")
```

```{r}
mpg_hp.res <- resid(mpg_hp.lm)
plot(density(mpg_hp.res), main = "Density Distribution of MPG ~ Horsepower")
```

```{r}
mpg_acc.res <- resid(mpg_acc.lm)
plot(density(mpg_acc.res), main = "Density Distribution of MPG ~ Acceleration")
```

<p style="color: red;">
They are all normally distributed and centered around zero.
</p>

## Reference:

[R SQUARED: SST, SSE AND SSR](https://rinterested.github.io/statistics/rsquare.html)

[The Correlation Coefficient (r)](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module9-Correlation-Regression/PH717-Module9-Correlation-Regression4.html)

[Significance Codes in R](https://www.statology.org/significance-codes-in-r/)